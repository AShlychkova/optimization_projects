{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust optimization\n",
    "### Shlychkova Aleksandra 699\n",
    "\n",
    "**Task:**\n",
    "In some optimization problems there is uncertainty or variation in the objective and constraint functions, due to parameters or factors that are either beyond our control or unknown. We can model this situation by making the objective and constraint functions $ f_0, ..., f_m$ functions of the optimization variable $x \\in R^n$ and a parameter vector $u \\in R^k$ that is unknown, or varies. In the stochastic optimization approach, the parameter vector u is modeled as a random variable with a known distribution, and we work with the expected values $E_u f_i(x, u)$. In the worst-case analysis approach, we are given a set $U$ that $u$  is know to lie in, and we work with the maximum or worst-case values $sup_{u \\in U} f_i(x, u)$. To simplify the discussion, we assume there are no equality constraints.\n",
    "\n",
    "\n",
    "[1] Stochastic optimization. We consider the problem:\n",
    "* minimize_ $(E f_0(x, u) )$\n",
    "* subject to_ $(Ef_i(x, u) < 0, i=1,..,m) $\n",
    "where the expectation is with respect to u. Show that if $f_i$ are convex in x for each $u$, then this stochastic optimization problem is convex.\n",
    "\n",
    "\n",
    "[2] Worst-case optimization. We consider the problem:\n",
    "* minimize  $sup_{u \\in U} f_0(x, u) $\n",
    "* subject to  $sup_{u \\in U} f_i(x, u) < 0, i=1,..,m, $\n",
    "Show that if $f_i$ are convex in x for each u, then this worst-case optimization problem is convex.\n",
    "\n",
    "\n",
    "[3] Finite set of possible parameter values. The observations made in parts (a) and (b) are most useful when we have analytical or easily evaluated expressions for the expected values $E f_i(x, u) $ or the worst-case values $sup_{u \\in U} f_i(x, u)$. Suppose we are given the set of possible values of the parameter is finite, i.e., we have $u \\in \\{u_1, ..., u_N\\}.$ For the stochastic case, we are also given the probabilities of each value: $prob(u=u_i)=p_i$, where $p\\in R^N, p > 0, 1^Tp=1$. Intheworst-case formulation, we simply take $U \\in \\{u_1, ..., u_N\\}$.\n",
    "Show how to set up the worst-case and stochastic optimization problems explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as sps\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent\n",
    "eps = 0.01\n",
    "\n",
    "def mod(x):\n",
    "    return (x**2).sum()**0.5\n",
    "\n",
    "def grad(f, x_0):\n",
    "    return (f(x_0)-f(x_0+eps))/eps\n",
    "\n",
    "def grad_min(f, x_0):\n",
    "    g = grad(f, x_0)\n",
    "    g = g/mod(g)\n",
    "    # check stopping criterion:\n",
    "    if mod(g):\n",
    "        return x_0\n",
    "    if (f(x_0)[1:]<0).any():\n",
    "        return x_0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E(f, x, U, PU):\n",
    "    def F(u):\n",
    "        return f(x, u)\n",
    "    F = np.vectorize(F, signature='()->()')\n",
    "    return F(U)*PU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
